{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# GPUs vs CPUs\n",
    "\n",
    "Source: [CUDA guide](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html)\n",
    "\n",
    "These are the basics of GPU programming needed to start making kernels with Triton.  Info is mostly from the CUDA guide linked above.\n",
    "\n",
    "CPUs execute tens of threads in parallel, and mainly do sequential computation.  GPUs execute thousands of threads in parallel.  This can be much faster, but requires a different programming model.\n",
    "\n",
    "![GPU threads](images/gpu_threads.png)\n",
    "\n",
    "The key with GPU programming is to read data into device memory, process it in parallel, and read it back out.  Reading data to/from the GPU is slow.  Algorithms like flash attention minimize copying and maximize compute utilization.\n",
    "\n",
    ">> Applications should strive to minimize data transfer between the host and the device. One way to accomplish this is to move more code from the host to the device, even if that means running kernels that do not expose enough parallelism to execute on the device with full efficiency. Intermediate data structures may be created in device memory, operated on by the device, and destroyed without ever being mapped by the host or copied to host memory.  Also, because of the overhead associated with each transfer, batching many small transfers into a single large transfer always performs better than making each transfer separately.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# GPU architecture\n",
    "\n",
    "A GPU is built around an array of Streaming Multiprocessors (SMs). A multithreaded program is partitioned into blocks of threads that execute independently from each other, so that a GPU with more multiprocessors will automatically execute the program in less time than a GPU with fewer multiprocessors.\n",
    "\n",
    "The NVIDIA GPU architecture is built around a scalable array of multithreaded Streaming Multiprocessors (SMs). When a CUDA program on the host CPU invokes a kernel grid, the blocks of the grid are enumerated and distributed to multiprocessors with available execution capacity. The threads of a thread block execute concurrently on one multiprocessor, and multiple thread blocks can execute concurrently on one multiprocessor. As thread blocks terminate, new blocks are launched on the vacated multiprocessors.\n",
    "\n",
    "The multiprocessor creates, manages, schedules, and executes threads in groups of 32 parallel threads called warps. Individual threads composing a warp start together at the same program address, but they have their own instruction address counter and register state and are therefore free to branch and execute independently. The term warp originates from weaving, the first parallel thread technology. A half-warp is either the first or second half of a warp. A quarter-warp is either the first, second, third, or fourth quarter of a warp.\n",
    "\n",
    "When a multiprocessor is given one or more thread blocks to execute, it partitions them into warps and each warp gets scheduled by a warp scheduler for execution. The way a block is partitioned into warps is always the same; each warp contains threads of consecutive, increasing thread IDs with the first warp containing thread 0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# CUDA\n",
    "\n",
    "CUDA helps you break tasks into small components that can be executed efficiently on GPUs.  You need to structure your code differently due to the number of threads.\n",
    "\n",
    "## Kernel grids and thread blocks\n",
    "\n",
    "CUDA C++ extends C++ by allowing the programmer to define C++ functions, called kernels, that, when called, are executed N times in parallel by N different CUDA threads, as opposed to only once like regular C++ functions.\n",
    "\n",
    "Kernels can include multiple thread blocks.  Here is a 1-d thread block:\n",
    "\n",
    "```\n",
    "// Kernel definition\n",
    "__global__ void VecAdd(float* A, float* B, float* C)\n",
    "{\n",
    "    int i = threadIdx.x;\n",
    "    C[i] = A[i] + B[i];\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    ...\n",
    "    // Kernel invocation with N threads\n",
    "    VecAdd<<<1, N>>>(A, B, C);\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "For convenience, threadIdx is a 3-component vector, so that threads can be identified using a one-dimensional, two-dimensional, or three-dimensional thread index, forming a one-dimensional, two-dimensional, or three-dimensional block of threads, called a thread block. This provides a natural way to invoke computation across the elements in a domain such as a vector, matrix, or volume.\n",
    "\n",
    "You can think of threadIdx as an index across one or more dimensions of your input data.  You're basically slicing your data into pieces, and operating on each piece individually.  This minimizes data copying and enables parallelism.\n",
    "\n",
    "Here is a 2-d thread block:\n",
    "\n",
    "```\n",
    "// Kernel definition\n",
    "__global__ void MatAdd(float A[N][N], float B[N][N],\n",
    "                       float C[N][N])\n",
    "{\n",
    "    int i = threadIdx.x;\n",
    "    int j = threadIdx.y;\n",
    "    C[i][j] = A[i][j] + B[i][j];\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    ...\n",
    "    // Kernel invocation with one block of N * N * 1 threads\n",
    "    int numBlocks = 1;\n",
    "    dim3 threadsPerBlock(N, N);\n",
    "    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "Thread ID is unique across the block.  The index of a thread and its thread ID relate to each other in a straightforward way: For a one-dimensional block, they are the same; for a two-dimensional block of size (Dx, Dy), the thread ID of a thread of index (x, y) is (x + y Dx); for a three-dimensional block of size (Dx, Dy, Dz), the thread ID of a thread of index (x, y, z) is (x + y Dx + z Dx Dy).\n",
    "\n",
    "There is a limit to the number of threads per block, since all threads of a block are expected to reside on the same streaming multiprocessor core and must share the limited memory resources of that core. On current GPUs, a thread block may contain up to 1024 threads.\n",
    "\n",
    "However, a kernel can be executed by multiple equally-shaped thread blocks, so that the total number of threads is equal to the number of threads per block times the number of blocks.\n",
    "\n",
    "Blocks are organized into a one-dimensional, two-dimensional, or three-dimensional grid of thread blocks. This is usually called a launch grid or a kernel grid. The number of thread blocks in a grid is usually dictated by the size of the data being processed, which typically exceeds the number of processors in the system.\n",
    "\n",
    "![Thread grid](images/grid-of-thread-blocks.png)\n",
    "\n",
    "Each block within the grid can be identified by a one-dimensional, two-dimensional, or three-dimensional unique index accessible within the kernel through the built-in blockIdx variable. The dimension of the thread block is accessible within the kernel through the built-in blockDim variable.\n",
    "\n",
    "Here is matrix addition with a launch grid of multiple thread blocks:\n",
    "\n",
    "```\n",
    "// Kernel definition\n",
    "__global__ void MatAdd(float A[N][N], float B[N][N],\n",
    "float C[N][N])\n",
    "{\n",
    "    int i = blockIdx.x * blockDim.x + threadIdx.x; //the current block index times the block number plus the current thread\n",
    "    int j = blockIdx.y * blockDim.y + threadIdx.y; //the current block index times the block number plus the current thread\n",
    "    if (i < N && j < N)\n",
    "        C[i][j] = A[i][j] + B[i][j]; // get a single element based on the thread block and thread index within the block\n",
    "}\n",
    "\n",
    "int main()\n",
    "{\n",
    "    ...\n",
    "    // Kernel invocation\n",
    "    dim3 threadsPerBlock(16, 16);\n",
    "    dim3 numBlocks(N / threadsPerBlock.x, N / threadsPerBlock.y);\n",
    "    MatAdd<<<numBlocks, threadsPerBlock>>>(A, B, C);\n",
    "    ...\n",
    "}\n",
    "```\n",
    "\n",
    "We basically split the input matrices into blocks, and operate on each block individually.  Within each block, we operate on elements individually.\n",
    "\n",
    "A thread block size of 16x16 (256 threads), although arbitrary in this case, is a common choice. The grid is created with enough blocks to have one thread per matrix element as before. For simplicity, this example assumes that the number of threads per grid in each dimension is evenly divisible by the number of threads per block in that dimension, although that need not be the case.\n",
    "\n",
    "Thread blocks are required to execute independently: It must be possible to execute them in any order, in parallel or in series. This independence requirement allows thread blocks to be scheduled in any order across any number of cores as illustrated by Figure 3, enabling programmers to write code that scales with the number of cores.\n",
    "\n",
    "Threads within a block can cooperate by sharing data through some shared memory and by synchronizing their execution to coordinate memory accesses. More precisely, one can specify synchronization points in the kernel by calling the __syncthreads() intrinsic function; __syncthreads() acts as a barrier at which all threads in the block must wait before any is allowed to proceed. Shared Memory gives an example of using shared memory. In addition to __syncthreads(), the Cooperative Groups API provides a rich set of thread-synchronization primitives.\n",
    "\n",
    "For efficient cooperation, the shared memory is expected to be a low-latency memory near each processor core (much like an L1 cache) and __syncthreads() is expected to be lightweight.\n",
    "\n",
    "## Thread block clusters\n",
    "\n",
    "With the introduction of NVIDIA Compute Capability 9.0, the CUDA programming model introduces an optional level of hierarchy called Thread Block Clusters that are made up of thread blocks. Similar to how threads in a thread block are guaranteed to be co-scheduled on a streaming multiprocessor, thread blocks in a cluster are also guaranteed to be co-scheduled on a GPU Processing Cluster (GPC) in the GPU.\n",
    "\n",
    "Similar to thread blocks, clusters are also organized into a one-dimension, two-dimension, or three-dimension as illustrated by Figure 5. The number of thread blocks in a cluster can be user-defined, and a maximum of 8 thread blocks in a cluster is supported as a portable cluster size in CUDA. Note that on GPU hardware or MIG configurations which are too small to support 8 multiprocessors the maximum cluster size will be reduced accordingly. Identification of these smaller configurations, as well as of larger configurations supporting a thread block cluster size beyond 8, is architecture-specific and can be queried using the cudaOccupancyMaxPotentialClusterSize API.\n",
    "\n",
    "![Cluster grid](images/grid-of-clusters.png)\n",
    "\n",
    "# Memory access\n",
    "\n",
    "Optimizing memory access is the most important part of optimizing a CUDA program.\n",
    "\n",
    "CUDA threads may access data from multiple memory spaces during their execution. Each thread has private local memory. Each thread block has shared memory visible to all threads of the block and with the same lifetime as the block. Thread blocks in a thread block cluster can perform read, write, and atomics operations on each other’s shared memory. All threads have access to the same global memory.\n",
    "\n",
    "![Memory](images/memory-hierarchy.png)\n",
    "\n",
    "There are also two additional read-only memory spaces accessible by all threads: the constant and texture memory spaces. The global, constant, and texture memory spaces are optimized for different memory usages (see Device Memory Accesses). Texture memory also offers different addressing modes, as well as data filtering, for some specific data formats (see Texture and Surface Memory).\n",
    "\n",
    "The global, constant, and texture memory spaces are persistent across kernel launches by the same application.\n",
    "\n",
    "Performance optimization revolves around four basic strategies:\n",
    "- Maximize parallel execution to achieve maximum utilization;\n",
    "- Optimize memory usage to achieve maximum memory throughput;\n",
    "- Optimize instruction usage to achieve maximum instruction throughput;\n",
    "- Minimize memory thrashing.\n",
    "\n",
    "The execution context (program counters, registers, and so on) for each warp processed by a multiprocessor is maintained on-chip during the entire lifetime of the warp. Therefore, switching from one execution context to another has no cost, and at every instruction issue time, a warp scheduler selects a warp that has threads ready to execute its next instruction (the active threads of the warp) and issues the instruction to those threads.\n",
    "\n",
    "In particular, each multiprocessor has a set of 32-bit registers that are partitioned among the warps, and a parallel data cache or shared memory that is partitioned among the thread blocks."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
