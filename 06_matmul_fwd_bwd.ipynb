{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76021418-c90d-442e-89db-df550aca545c",
   "metadata": {},
   "source": [
    "# Matmul forward and backward\n",
    "\n",
    "Source: [Triton matmul](https://triton-lang.org/main/getting-started/tutorials/03-matrix-multiplication.html#sphx-glr-getting-started-tutorials-03-matrix-multiplication-py)\n",
    "\n",
    "\n",
    "We can make a few improvements to our matmul implementation from last time:\n",
    "\n",
    "- Group rows for better performance\n",
    "- Add backward pass\n",
    "- Fuse in activation function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd6007c-5817-4a7f-a0d1-27924ca9a27b",
   "metadata": {},
   "source": [
    "## Grouping rows\n",
    "\n",
    "We can group blocks of rows together to ensure that a given block of columns is only loaded once for several blocks of rows (instead of loading a block of columns once per block of rows).\n",
    "\n",
    "Here's a diagram:\n",
    "\n",
    "![ordering](images/grouped_vs_row_major_ordering.png)\n",
    "\n",
    "This can improve performance, since memory access is a bottleneck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb097c22-6ee5-4151-9e0c-d28f6ddfbca5",
   "metadata": {},
   "source": [
    "## Fusing activation function\n",
    "\n",
    "We can fuse in an activation function by calling it from out matmul kernel.  We first define the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c3586e0-2221-44de-99e8-70a7db3cf5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.testing import assert_close\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "@triton.jit\n",
    "def relu(x, shape):\n",
    "    # Return the x value only when it's greater than 0\n",
    "    return tl.where(x > 0, x, 0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09cecb39-fe5e-4b5d-9b12-a4bf1708d8d8",
   "metadata": {},
   "source": [
    "Then, we can define our launch function for the matmul kernel.  We move to a 1-d grid so we have more control over the order of the x/y pids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66b32ea2-864f-4cf0-87df-56ec05d28e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def matmul_fwd(X, Y, activation=\"none\"):\n",
    "    x_rows, x_cols = X.shape\n",
    "    y_rows, y_cols = Y.shape\n",
    "    output = torch.zeros(x_rows, y_cols, device=\"cuda\", dtype=torch.float16) # Output matrix\n",
    "    \n",
    "    BLOCK_SIZE_X = 128\n",
    "    BLOCK_SIZE_Y = 128\n",
    "    BLOCK_SIZE_K = 32\n",
    "    GROUP_SIZE_X = 8\n",
    "    \n",
    "    # Create a 1-D grid to iterate across blocks\n",
    "    # We do 1-D so we can group rows more efficiently\n",
    "    grid = lambda meta: (triton.cdiv(x_rows, meta[\"BLOCK_SIZE_X\"]) * triton.cdiv(y_cols, meta[\"BLOCK_SIZE_Y\"]), )\n",
    "    \n",
    "    matmul_fwd_kernel[grid](X, Y, output, x_rows, x_cols, y_rows, y_cols, BLOCK_SIZE_X=BLOCK_SIZE_X, BLOCK_SIZE_Y=BLOCK_SIZE_Y, BLOCK_SIZE_K=BLOCK_SIZE_K, GROUP_SIZE_X=GROUP_SIZE_X, ACTIVATION=activation)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af9739c7-f049-4d77-b8a1-0904952fec51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def matmul_fwd_kernel(\n",
    "    x_ptr,\n",
    "    y_ptr,\n",
    "    output_ptr,\n",
    "    x_rows,\n",
    "    x_cols,\n",
    "    y_rows,\n",
    "    y_cols,\n",
    "    BLOCK_SIZE_X: tl.constexpr, # Row count per block\n",
    "    BLOCK_SIZE_Y: tl.constexpr, # column count per block\n",
    "    BLOCK_SIZE_K: tl.constexpr, # Inner dim to iterate over (count per iteration)\n",
    "    GROUP_SIZE_X: tl.constexpr, # Size of each X group\n",
    "    ACTIVATION: tl.constexpr, # Which activation function to use\n",
    "):\n",
    "    # The total pid count is number of x blocks times number of y blocks (total blocks to process)\n",
    "    # We get the pid of the current program\n",
    "    pid = tl.program_id(axis=0)\n",
    "\n",
    "    # Number of blocks across x\n",
    "    num_pid_x = tl.cdiv(x_rows, BLOCK_SIZE_X)\n",
    "\n",
    "    # Number of blocks across y\n",
    "    num_pid_y = tl.cdiv(y_cols, BLOCK_SIZE_Y)\n",
    "\n",
    "    # Total number of programs in each group\n",
    "    # Number of row groups times pid per column (we run once per row group/column block pair)\n",
    "    num_pid_in_group = GROUP_SIZE_X * num_pid_y\n",
    "\n",
    "    # Id of the group this program is in\n",
    "    # Divide current pid by number of pid in group\n",
    "    # Floor division gives the group id\n",
    "    group_id = pid // num_pid_in_group\n",
    "\n",
    "    # Row-id of the first program in the group\n",
    "    first_pid_x = group_id * GROUP_SIZE_X\n",
    "\n",
    "    # This is the size of the current group\n",
    "    # If `num_pid_x` isn't evenly divisible by `GROUP_SIZE_X`, the last group is smaller than the others\n",
    "    group_size_x = min(num_pid_x - first_pid_x, GROUP_SIZE_X)\n",
    "\n",
    "    # This is the x pid of the block to be retrived - we're going down the rows block by block before incrementing the y column\n",
    "    # So we multiply several blocks of rows by the same column block\n",
    "    # Row-id of the program in the *launch grid*\n",
    "    x_pid = first_pid_x + (pid % group_size_x)\n",
    "\n",
    "    # Find the pid within the current group\n",
    "    in_group_pid = (pid % num_pid_in_group) \n",
    "\n",
    "    # We only increment the y block when we've gone \"down\" the rows in x\n",
    "    # Column id of the program in the launch grid\n",
    "    y_pid = in_group_pid // group_size_x\n",
    "    \n",
    "    # Define the start position for the x and y pointers\n",
    "    # Remember that we have to stride across the columns when incrementing rows, and vice versa\n",
    "    x_row_start = x_pid * BLOCK_SIZE_X * x_cols # Start of the block we're selecting in x\n",
    "    y_col_start = y_pid * BLOCK_SIZE_Y # Start of block in y\n",
    "    \n",
    "    # Get the row and column offsets.  Row offsets need to be multiplied by the number of columns in the matrix\n",
    "    x_offsets = x_row_start + tl.arange(0, BLOCK_SIZE_X) * x_cols # Get row start index for each row (that's why we multiply by x_cols)\n",
    "    y_offsets = y_col_start + tl.arange(0, BLOCK_SIZE_Y) # Get column start index for each column (stride is 1)\n",
    "    \n",
    "    # Get the k offsets, for the matrix inner dimension\n",
    "    k_offsets = tl.arange(0, BLOCK_SIZE_K)\n",
    "    \n",
    "    # Define our x pointers, which will be from column 0 to k within each row\n",
    "    x_ptrs = x_ptr + (x_offsets[:,None] + k_offsets[None,:])\n",
    "    \n",
    "    # Define our y pointers, which will be from 0 to k within each column\n",
    "    # We multiply the k offsets by y_cols to get the row start positions\n",
    "    y_ptrs = y_ptr + (k_offsets[:,None] * y_cols + y_offsets[None,:])\n",
    "    \n",
    "    # The accumulator stores the results as we iterate across k\n",
    "    # Store in float32 for better numerical precision\n",
    "    accumulator = tl.zeros((BLOCK_SIZE_X, BLOCK_SIZE_Y), dtype=tl.float32)\n",
    "    # Iterate across k, increment by BLOCK_SIZE_K\n",
    "    for k in range(0, tl.cdiv(x_cols, BLOCK_SIZE_K)):\n",
    "        # Load the x subset to multiply\n",
    "        # We mask to avoid loading anything beyond the end of each column\n",
    "        # [None, :] adds a 1-length first dimension, so the mask can broadcast across x_ptrs\n",
    "        a = tl.load(x_ptrs, mask=k_offsets[None, :] < x_cols - k * BLOCK_SIZE_K, other=0.0)\n",
    "        \n",
    "        # [:, None] adds a 1-length second dimension, so the mask can broadcast across y_ptrs\n",
    "        b = tl.load(y_ptrs, mask=k_offsets[:,None] < y_rows - k * BLOCK_SIZE_K, other=0.0)\n",
    "        \n",
    "        # Multiply a and b, then add to the accumulator\n",
    "        result = tl.dot(a, b)\n",
    "        accumulator += result\n",
    "        \n",
    "        # Increment the x pointers to go across the rows\n",
    "        x_ptrs += BLOCK_SIZE_K\n",
    "        \n",
    "        # Increment the y pointers to go down the columns - we need to multiply by y_cols because we're moving down the columns (across the rows)\n",
    "        y_ptrs += BLOCK_SIZE_K * y_cols\n",
    "    \n",
    "    output = accumulator.to(tl.float16)\n",
    "    \n",
    "    # Add in the activation function\n",
    "    # Depending on the function, you can do this before/after the fp16 cast\n",
    "    if ACTIVATION == \"relu\":\n",
    "        output = relu(output, (BLOCK_SIZE_X, BLOCK_SIZE_Y))\n",
    "    \n",
    "    # Find the output pointer positions\n",
    "    output_x_start = x_pid * BLOCK_SIZE_X\n",
    "    output_y_start = y_pid * BLOCK_SIZE_Y\n",
    "    \n",
    "    output_x_rows = output_x_start + tl.arange(0, BLOCK_SIZE_X)\n",
    "    \n",
    "    output_x_offsets = output_x_start + tl.arange(0, BLOCK_SIZE_X)\n",
    "    output_y_offsets = output_y_start + tl.arange(0, BLOCK_SIZE_Y)\n",
    "    \n",
    "    # Store the data, ensuring we don't overflow the rows/columns\n",
    "    output_ptrs = output_ptr + (output_x_offsets[:, None] * y_cols + output_y_offsets[None, :])\n",
    "    output_mask = (output_x_rows[:, None] < x_rows) & (output_y_offsets[None,:] < y_cols)\n",
    "    tl.store(output_ptrs, output, mask=output_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0266e8a-4e39-4430-919c-b5e2a9395f9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum difference between torch and triton is 0.0\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  5.2227,  0.0000],\n",
      "        [ 2.6484,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 5.5938,  0.0000,  0.0000,  2.3438,  9.6562],\n",
      "        [ 0.0000,  3.9062,  0.0000,  0.0000,  0.0000],\n",
      "        [14.8203,  8.7969,  0.0000,  0.2194,  0.0000]], device='cuda:0',\n",
      "       dtype=torch.float16)\n",
      "tensor([[ 0.0000,  0.0000,  0.0000,  5.2227,  0.0000],\n",
      "        [ 2.6484,  0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 5.5938,  0.0000,  0.0000,  2.3438,  9.6562],\n",
      "        [ 0.0000,  3.9062,  0.0000,  0.0000,  0.0000],\n",
      "        [14.8203,  8.7969,  0.0000,  0.2194,  0.0000]], device='cuda:0',\n",
      "       dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.rand((8192, 8192), device='cuda', dtype=torch.float16) - .5\n",
    "y = torch.rand((8192, 8192), device='cuda', dtype=torch.float16) - .5\n",
    "\n",
    "act = nn.ReLU()\n",
    "output_torch = act(x @ y)\n",
    "output_triton = matmul_fwd(x, y, activation=\"relu\")\n",
    "print(\n",
    "    f'The maximum difference between torch and triton is '\n",
    "    f'{torch.max(torch.abs(output_torch - output_triton))}'\n",
    ")\n",
    "print(output_torch[:5,:5])\n",
    "print(output_triton[:5,:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1d41b3-97bf-498c-87cf-113cdb8d5799",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Adding the backward pass\n",
    "\n",
    "For `Z = X @ Y`, the gradient wrt `X` is `Z @ Y.T`, and the gradient wrt `Y` is `X.T @ Z`.  We can call the matmul kernel twice to calculate these. We could do additional optimization by only loading `Z` once per group for both computations.\n",
    "\n",
    "First, we'll do the same operation in torch to benchmark:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7130e6a2-604d-42e1-b025-d9e5e17c9dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand((8192, 8192), device='cuda', dtype=torch.float16) - .5\n",
    "y = torch.rand((8192, 8192), device='cuda', dtype=torch.float16) - .5\n",
    "\n",
    "x.requires_grad = True\n",
    "y.requires_grad = True\n",
    "\n",
    "output_torch = act(x @ y)\n",
    "output_torch = F.softmax(output_torch, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1c71e16-308c-419d-9b45-4272935739d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = torch.zeros_like(x)\n",
    "inds = (torch.arange(0, target.shape[0]), torch.randint(0, x.shape[1], (target.shape[0],)))\n",
    "target[inds] = 1\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(output_torch, target)\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f3b4fe09-04a4-4475-b1d4-1e17599714e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_xgrad = x.grad.detach().clone()\n",
    "torch_ygrad = y.grad.detach().clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da448436-a5b4-4667-8d73-6601fc962f08",
   "metadata": {},
   "source": [
    "Then, we can define our matmul kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "697b2e71-89ee-48ad-9926-1f40f40995e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatMul(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, y):\n",
    "        activation = \"relu\"\n",
    "        z = matmul_fwd(x, y, activation)\n",
    "        \n",
    "        ctx.save_for_backward(x, y, z) # cache x and y for use in backward pass\n",
    "        ctx.activation = activation\n",
    "        return z\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dz):\n",
    "        x, y, z = ctx.saved_tensors # grab x and y from the cache\n",
    "        \n",
    "        # Apply relu backwards\n",
    "        # Would be more efficient to fuse this in\n",
    "        if ctx.activation == \"relu\":\n",
    "            dz = torch.where(z > 0, dz, 0.0)\n",
    "        \n",
    "        # Would be more efficient to do a single matmul call here\n",
    "        dx = matmul_fwd(dz, y.T, activation=\"none\")\n",
    "        dy = matmul_fwd(x.T, dz, activation=\"none\")\n",
    "        \n",
    "        return dx, dy\n",
    "\n",
    "matmul = MatMul.apply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34eda353-1ed4-45aa-b2fc-a33026a1f962",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad.data.zero_() # zero out the x gradient\n",
    "y.grad.data.zero_() # zero out y gradient\n",
    "\n",
    "output_triton = matmul(x, y)\n",
    "output_triton = F.softmax(output_triton, dim=-1)\n",
    "\n",
    "triton_loss = loss_fn(output_triton, target)\n",
    "triton_loss.backward()\n",
    "assert_close(x.grad, torch_xgrad, atol=1e-2, rtol=0)\n",
    "assert_close(y.grad, torch_ygrad, atol=1e-2, rtol=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
