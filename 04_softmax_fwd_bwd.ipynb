{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9fe5e826-42b1-4190-b41a-9ce9b2ffd9ce",
   "metadata": {},
   "source": [
    "# Softmax forward and backward pass\n",
    "\n",
    "This is an (inefficient) softmax implementation designed to help understand the forward and backward pass of triton.  It works by fitting an entire row into one block for both the forward and backward passes.\n",
    "\n",
    "It's loosely based on [layernorm](https://triton-lang.org/main/getting-started/tutorials/05-layer-norm.html) and [softmax](https://triton-lang.org/main/getting-started/tutorials/02-fused-softmax.html) Triton tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386215d8-58fe-43a7-95d3-904d8186912a",
   "metadata": {},
   "source": [
    "First, we define the forward kernel.  Remember, softmax is $sigma(z_i) = \\frac{e^{z_{i}}}{\\sum_{j=1}^K e^{z_{j}}}$.\n",
    "\n",
    "1. Load in a row of data and define the mask.\n",
    "2. Subtract the maximum of the row from each value in the row for numerical stability (prevent overflows with $e^{z_{i}}$)\n",
    "3. Implement the softmax equation (raise e to the power of the x values, divide by the sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3e6ab693-44cd-44d6-906e-839b81450714",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.testing import assert_close\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import triton\n",
    "import triton.language as tl\n",
    "\n",
    "\n",
    "@triton.jit\n",
    "def softmax_fwd_kernel(\n",
    "    x_ptr,\n",
    "    output_ptr,\n",
    "    x_rows,\n",
    "    x_cols,\n",
    "    BLOCK_SIZE:tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(axis=0) # Get the current row id\n",
    "    \n",
    "    row_start = pid * x_cols\n",
    "    row_offset = row_start + tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    mask = row_offset < row_start + x_cols\n",
    "    \n",
    "    row = tl.load(x_ptr + row_offset, mask=mask, other=-float('inf'))\n",
    "    \n",
    "    row = row - tl.max(row, axis=0) # Subtract max for stability\n",
    "    num = tl.exp(row) # Raise e to the power of each element\n",
    "    denom = tl.sum(num, axis=0) # Sum the elements in the row\n",
    "    output = num / denom #Softmax output\n",
    "    \n",
    "    tl.store(output_ptr + row_offset, output, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae7ad7e0-5fe7-4136-b4eb-36d2ff1b45f8",
   "metadata": {},
   "source": [
    "Then, we can define the softmax forward python function.  We operate on each row.  We have to allocate a block that covers all the elements in the row, but Triton requires that we go to the next power of 2.\n",
    "\n",
    "We create a 1-d launch grid over the rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d4d8023-ccf5-47b5-9824-642b1d7f8abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax_fwd(x):\n",
    "    output = torch.empty_like(x)\n",
    "    x_rows, x_cols = x.shape\n",
    "    \n",
    "    # Set the block size - Triton only works with powers of 2\n",
    "    BLOCK_SIZE = triton.next_power_of_2(x_cols)\n",
    "    # 1-d launch grid\n",
    "    grid = lambda meta: (x_rows,)\n",
    "    \n",
    "    softmax_fwd_kernel[grid](x, output, x_rows, x_cols, BLOCK_SIZE=BLOCK_SIZE)\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ca6d8f-fa64-483c-8c71-4a600264a2ec",
   "metadata": {},
   "source": [
    "We can test our softmax kernel against the torch implementation using `assert_close`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a24599c9-1b3c-4b9c-aa7d-f68c70c42cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "x = torch.rand((10, 6), device='cuda')\n",
    "\n",
    "output_triton = softmax_fwd(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac05b13a-535b-4a20-a633-dbdd4d5a40ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad = True\n",
    "output_torch = F.softmax(x, dim=-1)\n",
    "\n",
    "assert_close(output_triton, output_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b96a27cf-9200-40cb-8053-4eb9f1e91394",
   "metadata": {},
   "source": [
    "We set `requires_grad` for `x` above - this will allow us to run a backward pass with our torch tensor.  We can define a fake one-hot encoded target, y, and use [CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) to find the gradient wrt `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a705a9f4-b664-418e-b91a-7c8b54d437fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup a fake target\n",
    "y = torch.zeros_like(x)\n",
    "inds = (torch.arange(0, y.shape[0]), torch.randint(0, 3, (y.shape[0],)))\n",
    "y[inds] = 1\n",
    "\n",
    "# Define loss and run backward pass\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "loss = loss_fn(output_torch, y)\n",
    "loss.backward()\n",
    "\n",
    "# Save gradient tensor for later\n",
    "torch_xgrad = x.grad.detach().clone()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bbbb69-0446-45d3-83aa-0a154e86698b",
   "metadata": {},
   "source": [
    "We can now write the softmax backward kernel to compare to torch.  This is a very inefficient implementation for learning purposes only!  We break down the softmax into a computational graph, and then run the backward pass against the graph, step by step.\n",
    "\n",
    "This is the forward computational graph for softmax:\n",
    "\n",
    "![comp graph](images/comp_graph.png)\n",
    "\n",
    "1. Run the forward pass again to regenerate values needed to compute backward step.\n",
    "2. Compute gradient step by step (see comments).\n",
    "3. Store the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "87418d8d-17e9-47f3-82f2-9b5409bf7252",
   "metadata": {},
   "outputs": [],
   "source": [
    "@triton.jit\n",
    "def softmax_bwd_kernel(\n",
    "    x_ptr,\n",
    "    dy_ptr,\n",
    "    dx_ptr,\n",
    "    x_rows,\n",
    "    x_cols,\n",
    "    BLOCK_SIZE: tl.constexpr\n",
    "):\n",
    "    pid = tl.program_id(axis=0) # Get the current row id\n",
    "    \n",
    "    row_start = pid * x_cols # Get start of row that was softmaxed\n",
    "    row_offset = row_start + tl.arange(0, BLOCK_SIZE)\n",
    "    \n",
    "    mask = row_offset < row_start + x_cols\n",
    "    \n",
    "    # Get x row and incoming gradient row\n",
    "    x_row = tl.load(x_ptr + row_offset, mask=mask, other=-float('inf'))\n",
    "    dy_row = tl.load(dy_ptr + row_offset, mask=mask, other=0.0)\n",
    "    \n",
    "    x_normed = x_row - tl.max(x_row, axis=0) # Subtract max for stability\n",
    "    \n",
    "    num = tl.exp(x_normed) # Raise e to the power of each element\n",
    "    denom = tl.sum(num, axis=0) # Sum the elements in the row\n",
    "    \n",
    "    # This is modified slightly to make the derivative easier to understand\n",
    "    inverted = 1 / denom # Invert denominator\n",
    "    output = num * inverted # Multiply numerator by inverted denominator\n",
    "    \n",
    "    # Find gradient for both components of final softmax forward step\n",
    "    num_grad = dy_row * inverted\n",
    "    inverted_grad = dy_row * num\n",
    "    inverted_grad = tl.sum(inverted_grad, axis=0)\n",
    "    \n",
    "    # Gradient wrt to softmax denominator\n",
    "    denom_grad = (-1 / (denom * denom)) * inverted_grad\n",
    "    \n",
    "    # Sum grad from denominator (which is the sum of numerator) into the numerator grad\n",
    "    num_grad += tl.full(num_grad.shape, 1.0, dtype=tl.float32) * denom_grad\n",
    "    \n",
    "    # Find gradient wrt the normalized xs\n",
    "    normed_grad = num_grad * tl.exp(x_normed)\n",
    "    \n",
    "    # Find the gradient across the normalization\n",
    "    x_grad = normed_grad\n",
    "    max_grad = -normed_grad\n",
    "    max_grad = tl.sum(max_grad, axis=0)\n",
    "    x_grad_2 = max_grad * tl.where(x_row == tl.max(x_row, axis=0), tl.full(x_grad.shape, 1.0, dtype=tl.float32), tl.zeros(x_grad.shape, dtype=tl.float32))\n",
    "   \n",
    "    tl.store(dx_ptr + row_offset, x_grad + x_grad_2, mask=mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we create a torch autograd function that lets us define a custom forward and backward pass:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "38178563-5b0d-430b-a2f9-4d8e6894e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax(torch.autograd.Function):\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        output = torch.empty_like(x)\n",
    "        x_rows, x_cols = x.shape\n",
    "\n",
    "        BLOCK_SIZE = triton.next_power_of_2(x_cols)\n",
    "        grid = lambda meta: (x_rows,)\n",
    "\n",
    "        softmax_fwd_kernel[grid](x, output, x_rows, x_cols, BLOCK_SIZE=BLOCK_SIZE)\n",
    "        \n",
    "        ctx.save_for_backward(x) # cache x for use in backward pass\n",
    "        return output\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, dy):\n",
    "        x = ctx.saved_tensors[0] # grab x from the cache\n",
    "        dx = torch.empty_like(x)\n",
    "        x_rows, x_cols = x.shape\n",
    "\n",
    "        BLOCK_SIZE = triton.next_power_of_2(x_cols)\n",
    "        grid = lambda meta: (x_rows,)\n",
    "\n",
    "        softmax_bwd_kernel[grid](x, dy, dx, x_rows, x_cols, BLOCK_SIZE=BLOCK_SIZE)\n",
    "\n",
    "        return dx\n",
    "\n",
    "\n",
    "softmax = Softmax.apply"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can test it by checking the forward pass:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d9d478a0-a7ed-4954-ba2d-7c2ed15528ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "x.grad.data.zero_() # zero out the x gradient\n",
    "\n",
    "output_triton2 = softmax(x)\n",
    "\n",
    "assert_close(output_triton, output_triton2)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "And the backward pass:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1ff4dd7-6167-436e-a5af-3861fc60b8a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0160,  0.0163, -0.0846,  0.0176,  0.0176,  0.0171],\n",
      "        [-0.0835,  0.0178,  0.0160,  0.0182,  0.0158,  0.0157],\n",
      "        [ 0.0162,  0.0180, -0.0842,  0.0158,  0.0177,  0.0163],\n",
      "        [-0.0838,  0.0166,  0.0176,  0.0159,  0.0174,  0.0163],\n",
      "        [ 0.0158, -0.0831,  0.0158,  0.0163,  0.0182,  0.0170],\n",
      "        [-0.0840,  0.0172,  0.0174,  0.0181,  0.0156,  0.0157],\n",
      "        [ 0.0169, -0.0823,  0.0160,  0.0157,  0.0171,  0.0166],\n",
      "        [ 0.0161, -0.0825,  0.0186,  0.0158,  0.0163,  0.0158],\n",
      "        [ 0.0171, -0.0833,  0.0155,  0.0176,  0.0166,  0.0165],\n",
      "        [-0.0842,  0.0161,  0.0167,  0.0178,  0.0178,  0.0159]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "triton_loss = loss_fn(output_triton2, y)\n",
    "triton_loss.backward()\n",
    "assert_close(x.grad, torch_xgrad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
